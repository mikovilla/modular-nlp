{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5112e5a6-b2ea-4f21-8cbf-764c7e1cff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e8ea975736470e9dc0516933d5e171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29a757d111b497e917157bb27fd89d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b837633cae6d42b1ae7090fa16ea304d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG-LIVE] epoch_start=0 global_step=0 wrapped=AcceleratedOptimizer base=AdamW lr=2e-05 id=136365644563424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/45 00:21 < 00:33, 0.78 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.053314</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG-LIVE] epoch_start=1.0 global_step=9 wrapped=AcceleratedOptimizer base=AdamW lr=2e-05 id=136365644563424\n",
      "Switching optimizer to SGD right after epoch 2 (next epoch will print SGD in your debug)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SwitchOptimizerCallback' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m helper.list_config()\n\u001b[32m     18\u001b[39m mbert_context = context.setup_pipeline(MBertConfig, require_translation = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m mbert_trainer = \u001b[43msentiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmbert_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#mamba_context = context.setup_pipeline(MambaConfig, require_translation = True)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#mamba_trainer = sentiment.train(mamba_context)\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# TBA: FUSION #\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AppConfig.INFER_SAMPLE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Miko/.dev/xl_nlp/src/sentiment.py:66\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(context)\u001b[39m\n\u001b[32m     63\u001b[39m     dbg.bind(trainer)\n\u001b[32m     64\u001b[39m     trainer.add_callback(dbg)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m eval_metrics = trainer.evaluate(eval_dataset=context.test_ds)\n\u001b[32m     68\u001b[39m helper.print_header(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext.modelConfig.MODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m evaluation metrics\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mv_nlp/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mv_nlp/lib/python3.12/site-packages/transformers/trainer.py:2787\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2780\u001b[39m     logger.warning(\n\u001b[32m   2781\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThere seems not to be a single sample in your epoch_iterator, stopping training at step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2782\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m! This is expected if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using an IterableDataset and set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2783\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m num_steps (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) higher than the number of available samples.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2784\u001b[39m     )\n\u001b[32m   2785\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2788\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_log_save_evaluate(\n\u001b[32m   2789\u001b[39m     tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=learning_rate\n\u001b[32m   2790\u001b[39m )\n\u001b[32m   2792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mv_nlp/lib/python3.12/site-packages/transformers/trainer_callback.py:516\u001b[39m, in \u001b[36mCallbackHandler.on_epoch_end\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_epoch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mv_nlp/lib/python3.12/site-packages/transformers/trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Miko/.dev/xl_nlp/src/optimizer.py:48\u001b[39m, in \u001b[36mSwitchOptimizerCallback.on_epoch_end\u001b[39m\u001b[34m(self, args, state, control, model, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.create_scheduler(num_training_steps=steps, optimizer=\u001b[38;5;28mself\u001b[39m.trainer.optimizer)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.optimizer.param_groups:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     g[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 5) Sanity print (live)\u001b[39;00m\n\u001b[32m     51\u001b[39m base = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.trainer.optimizer, \u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.trainer.optimizer)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SwitchOptimizerCallback' object has no attribute 'lr'"
     ]
    }
   ],
   "source": [
    "# Generic: A Modular Multi-Pipeline Framework for Probability Fusion Ensembles\n",
    "# Specific: Cross-Lingual Sentiment Analysis with Probability Fusion Ensembles: A Modular Multi-Pipeline Framework for Low-Resource Languages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "\n",
    "from src import sentiment, helper, utility, context\n",
    "from src.config import *\n",
    "from src.metrics import evaluate_pipe\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "helper.list_config()\n",
    "\n",
    "mbert_context = context.setup_pipeline(MBertConfig, require_translation = False)\n",
    "mbert_trainer = sentiment.train(mbert_context)\n",
    "\n",
    "#mamba_context = context.setup_pipeline(MambaConfig, require_translation = True)\n",
    "#mamba_trainer = sentiment.train(mamba_context)\n",
    "\n",
    "# TBA: FUSION #\n",
    "\n",
    "if AppConfig.INFER_SAMPLE:\n",
    "    sample_texts = [\n",
    "        \"Maganda ang serbisyo at mabilis ang delivery!\"\n",
    "        \"Sobrang pangit ng karanasan ko.\",\n",
    "        \"It was okay, nothing special.\",\n",
    "    ]\n",
    "    #sentiment.infer(sample_texts, mbert_trainer.tokenizer, mbert_trainer.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a1294-52f8-4873-9763-d2ad352c2652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
