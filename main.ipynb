{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112e5a6-b2ea-4f21-8cbf-764c7e1cff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miko/miniconda/envs/mv_nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/miko/miniconda/envs/mv_nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████| 69/69 [00:00<00:00, 17908.85 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 4788.02 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 8935.82 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/miko/miniconda/envs/mv_nlp/lib/python3.12/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "\n",
    "from src import sentiment, helper, utility, models\n",
    "from src.config import MBertConfig, SharedConfig\n",
    "from src.metrics import evaluate_pipe\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # helper.jsonToCSV(\"miko.json\", \"miko.csv\")\n",
    "    \n",
    "    jsonl = helper.read_jsonl_as_string(Path(\"miko.jsonl\"))\n",
    "    train_ds, val_ds, test_ds, label2id, id2label = utility.load_split_dataset(jsonl)  # load again, un-mapped\n",
    "\n",
    "    texts = helper.to_list_str(test_ds[SharedConfig.TEXT_COL])\n",
    "    labels = list(test_ds[SharedConfig.LABEL_COL])\n",
    "    num_labels = len(id2label)\n",
    "\n",
    "    context = models.setup_pipeline(MBertConfig)\n",
    "    \n",
    "    mBertTrainer = sentiment.train(context)\n",
    "    mBertPipe = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=mBertTrainer.model, \n",
    "        tokenizer=mBertTrainer.tokenizer, \n",
    "        return_all_scores=True,\n",
    "        device=0 if SharedConfig.USE_FP16 else -1\n",
    "    )\n",
    "    metrics = evaluate_pipe(mBertPipe, texts, labels, id2label=mBertTrainer.model.config.id2label)\n",
    "    print(\"\\r\\nMetrics:\")\n",
    "    pprint.pprint(metrics)\n",
    "    \n",
    "    #sample_texts = [\n",
    "    #    \"Maganda ang serbisyo at mabilis ang delivery!\",  # Tagalog positive\n",
    "    #    \"Sobrang pangit ng karanasan ko.\",                # Tagalog negative\n",
    "    #    \"It was okay, nothing special.\",                  # English neutral-ish\n",
    "    #]\n",
    "    #sentiment.infer(sample_texts, trainer.tokenizer, trainer.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb97d01-9d1d-4ffd-a29c-48069d8a7266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
