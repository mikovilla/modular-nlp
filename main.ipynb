{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5112e5a6-b2ea-4f21-8cbf-764c7e1cff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1565db6c1803448eb663b0bdeee9f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eec675e0f74dae9d7d6ee1c55f6888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac447f0d2c4b83a9906a979c1cae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataloader_num_workers': 1, 'eval_strategy': 'epoch', 'fp16': True, 'gradient_accumulation_steps': 2, 'greater_is_better': True, 'learning_rate': 2e-05, 'load_best_model_at_end': True, 'logging_steps': 50, 'logging_strategy': 'steps', 'lr_scheduler_type': 'linear', 'max_grad_norm': 1.0, 'metric_for_best_model': 'eval_f1_macro', 'num_train_epochs': 15, 'output_dir': './mbert_sentiment', 'per_device_eval_batch_size': 32, 'per_device_train_batch_size': 8, 'save_strategy': 'epoch', 'seed': 42, 'warmup_ratio': 0.06, 'weight_decay': 0.01}\n",
      "[OPTIMIZER] epoch_start=0 global_step=0 wrapped=AcceleratedOptimizer base=AdamW lr=0.0 id=126607492403168 hyperparams={'lr': 0.0, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 0.0, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[0.0]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 04:56, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.052214</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.543087</td>\n",
       "      <td>0.578704</td>\n",
       "      <td>0.564815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.601852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755001</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.463203</td>\n",
       "      <td>0.440351</td>\n",
       "      <td>0.546296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.791855</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.521056</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.578830</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.803947</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.562891</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.870287</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.571500</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.870287</td>\n",
       "      <td>0.886905</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.715317</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.807928</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.807928</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.648441</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.837117</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.718314</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866318</td>\n",
       "      <td>0.874644</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.719336</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866318</td>\n",
       "      <td>0.874644</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.723765</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866318</td>\n",
       "      <td>0.874644</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.730074</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866318</td>\n",
       "      <td>0.874644</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING_PERFORMANCE] epoch=1.0 tokens=0 time=3.18s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=2.00e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.04s tok/s=225618.9 ex/s=1762.65\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.6000 cost=$0.00 accuracy_per_$=314.82\n",
      "[OPTIMIZER] epoch_start=1.0 global_step=14 wrapped=AcceleratedOptimizer base=AdamW lr=2e-05 id=126607492403168 hyperparams={'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[2e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=2.0 tokens=0 time=2.27s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.87e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.02s tok/s=348808.6 ex/s=2725.07\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.6000 cost=$0.01 accuracy_per_$=43.30\n",
      "[OPTIMIZER] epoch_start=2.0 global_step=28 wrapped=AcceleratedOptimizer base=AdamW lr=1.868020304568528e-05 id=126607492403168 hyperparams={'lr': 1.868020304568528e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.868020304568528e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.868020304568528e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=3.0 tokens=0 time=2.28s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.73e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=267531.5 ex/s=2090.09\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.5667 cost=$0.03 accuracy_per_$=21.73\n",
      "[OPTIMIZER] epoch_start=3.0 global_step=42 wrapped=AcceleratedOptimizer base=AdamW lr=1.7258883248730966e-05 id=126607492403168 hyperparams={'lr': 1.7258883248730966e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.7258883248730966e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.7258883248730966e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=4.0 tokens=0 time=2.43s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.58e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.02s tok/s=336485.5 ex/s=2628.79\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8000 cost=$0.04 accuracy_per_$=20.29\n",
      "[OPTIMIZER] epoch_start=4.0 global_step=56 wrapped=AcceleratedOptimizer base=AdamW lr=1.583756345177665e-05 id=126607492403168 hyperparams={'lr': 1.583756345177665e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.583756345177665e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.583756345177665e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=5.0 tokens=0 time=2.45s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.44e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.04s tok/s=207811.1 ex/s=1623.52\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.05 accuracy_per_$=16.76\n",
      "[OPTIMIZER] epoch_start=5.0 global_step=70 wrapped=AcceleratedOptimizer base=AdamW lr=1.4416243654822337e-05 id=126607492403168 hyperparams={'lr': 1.4416243654822337e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.4416243654822337e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.4416243654822337e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=6.0 tokens=0 time=2.40s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.30e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.04s tok/s=228374.0 ex/s=1784.17\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8000 cost=$0.06 accuracy_per_$=12.42\n",
      "[OPTIMIZER] epoch_start=6.0 global_step=84 wrapped=AcceleratedOptimizer base=AdamW lr=1.2994923857868021e-05 id=126607492403168 hyperparams={'lr': 1.2994923857868021e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.2994923857868021e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.2994923857868021e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=7.0 tokens=0 time=2.23s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.17e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=255965.9 ex/s=1999.73\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.08 accuracy_per_$=11.43\n",
      "[OPTIMIZER] epoch_start=7.0 global_step=98 wrapped=AcceleratedOptimizer base=AdamW lr=1.16751269035533e-05 id=126607492403168 hyperparams={'lr': 1.16751269035533e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.16751269035533e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.16751269035533e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=8.0 tokens=0 time=2.37s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=1.03e-05\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.02s tok/s=342114.3 ex/s=2672.77\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.09 accuracy_per_$=9.92\n",
      "[OPTIMIZER] epoch_start=8.0 global_step=112 wrapped=AcceleratedOptimizer base=AdamW lr=1.0253807106598985e-05 id=126607492403168 hyperparams={'lr': 1.0253807106598985e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 1.0253807106598985e-05, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.0253807106598985e-05]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=9.0 tokens=0 time=2.46s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=8.83e-06\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=249579.6 ex/s=1949.84\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8000 cost=$0.10 accuracy_per_$=8.10\n",
      "[OPTIMIZER] epoch_start=9.0 global_step=126 wrapped=AcceleratedOptimizer base=AdamW lr=8.832487309644671e-06 id=126607492403168 hyperparams={'lr': 8.832487309644671e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 8.832487309644671e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[8.832487309644671e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=10.0 tokens=0 time=2.52s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=7.41e-06\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=257340.5 ex/s=2010.47\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8000 cost=$0.11 accuracy_per_$=7.23\n",
      "[OPTIMIZER] epoch_start=10.0 global_step=140 wrapped=AcceleratedOptimizer base=AdamW lr=7.411167512690356e-06 id=126607492403168 hyperparams={'lr': 7.411167512690356e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 7.411167512690356e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[7.411167512690356e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=11.0 tokens=0 time=2.23s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=5.99e-06\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=305496.9 ex/s=2386.69\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8333 cost=$0.12 accuracy_per_$=6.85\n",
      "[OPTIMIZER] epoch_start=11.0 global_step=154 wrapped=AcceleratedOptimizer base=AdamW lr=5.989847715736041e-06 id=126607492403168 hyperparams={'lr': 5.989847715736041e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 5.989847715736041e-06, 'betas': (0.9, 0.999), 'eps': 1e-06, 'weight_decay': 0.01, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': True, 'initial_lr': 2e-05}\n",
      "[SCHEDULER]   scheduler_last_lr=[5.989847715736041e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "Switching optimizer to SGD right after epoch 12 (next epoch will print SGD in your debug)\n",
      "[OPTIMIZER_SWITCH] base=SGD lr=0.0001\n",
      "[TRAINING_PERFORMANCE] epoch=12.0 tokens=0 time=2.29s tok/s=0.0 ex/s=0.00 peak_mem=3.33GiB lr=4.57e-06\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.04s tok/s=230349.3 ex/s=1799.60\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.13 accuracy_per_$=6.56\n",
      "[OPTIMIZER] epoch_start=12.0 global_step=168 wrapped=AcceleratedOptimizer base=SGD lr=0.0001 id=126607489809488 hyperparams={'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[SCHEDULER]   scheduler_last_lr=[4.568527918781726e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=13.0 tokens=0 time=2.26s tok/s=0.0 ex/s=0.00 peak_mem=4.00GiB lr=1.00e-04\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=302235.8 ex/s=2361.22\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.14 accuracy_per_$=6.34\n",
      "[OPTIMIZER] epoch_start=13.0 global_step=182 wrapped=AcceleratedOptimizer base=SGD lr=0.0001 id=126607489809488 hyperparams={'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[SCHEDULER]   scheduler_last_lr=[3.1472081218274113e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=14.0 tokens=0 time=2.10s tok/s=0.0 ex/s=0.00 peak_mem=3.99GiB lr=1.00e-04\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=271362.3 ex/s=2120.02\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.14 accuracy_per_$=6.01\n",
      "[OPTIMIZER] epoch_start=14.0 global_step=196 wrapped=AcceleratedOptimizer base=SGD lr=0.0001 id=126607489809488 hyperparams={'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[OPTIMIZER_GROUP]   group[0] {'lr': 0.0001, 'momentum': 0.95, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "[SCHEDULER]   scheduler_last_lr=[1.7258883248730964e-06]\n",
      "[PARAMS]   params_total=177,855,747 params_trainable=177,855,747\n",
      "[TRAINING_PERFORMANCE] epoch=15.0 tokens=0 time=2.19s tok/s=0.0 ex/s=0.00 peak_mem=3.99GiB lr=1.00e-04\n",
      "[EVALUATION_PERFORMANCE] tokens=8192 time=0.03s tok/s=270141.0 ex/s=2110.48\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8667 cost=$0.15 accuracy_per_$=5.69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVALUATION_PERFORMANCE] tokens=16384 time=0.06s tok/s=276678.7 ex/s=2161.55\n",
      "[EVALUATION_PERFORMANCE] accuracy=0.8226 cost=$0.16 accuracy_per_$=5.06\n",
      "\n",
      "### BERT-BASE-MULTILINGUAL-CASED EVALUATION METRICS ###\n",
      "{'epoch': 15.0,\n",
      " 'eval_accuracy': 0.8225806451612904,\n",
      " 'eval_f1_macro': 0.8146523716699156,\n",
      " 'eval_loss': 0.7139000296592712,\n",
      " 'eval_precision_macro': 0.8307838561922046,\n",
      " 'eval_recall_macro': 0.8095906432748537,\n",
      " 'eval_runtime': 0.126,\n",
      " 'eval_samples_per_second': 491.91,\n",
      " 'eval_steps_per_second': 15.868}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebedfa6346bc438da4e3c8c73cad0da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f074ea2b72464a5dbc5af0e654dc6abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdb294a526141fab37248c118df900c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generic: A Modular Multi-Pipeline Framework for Probability Fusion Ensembles\n",
    "# Specific: Cross-Lingual Sentiment Analysis with Probability Fusion Ensembles: A Modular Multi-Pipeline Framework for Low-Resource Languages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "\n",
    "from src.config import *\n",
    "from src.metrics import evaluate_pipe\n",
    "from src import (\n",
    "    context,\n",
    "    helper,\n",
    "    sentiment, \n",
    "    utility, \n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "helper.list_config()\n",
    "\n",
    "if App.HAS_GPU:\n",
    "    os.environ[\"MAMBA_USE_MAMBAPY\"] = Mamba.FORCE_CUDA\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "if App.ACTION == \"INFER\":\n",
    "    sample_texts = [\n",
    "        \"Maganda ang serbisyo at mabilis ang delivery!\",\n",
    "        \"Sobrang pangit ng karanasan ko.\",\n",
    "        \"It was okay, nothing special.\",\n",
    "    ]\n",
    "    sentiment.infer(sample_texts, MBert)\n",
    "elif App.ACTION == \"ENSEMBLE\":\n",
    "        temps  = [1.1, 0.9]\n",
    "        weights = [0.4, 0.6]\n",
    "        ens = sentiment.ensemble([MBert, Mamba], temps, weights)\n",
    "        print(ens)\n",
    "elif App.ACTION == \"TRAIN\":\n",
    "    mbert_context = context.setup_pipeline(MBert, require_translation = False)\n",
    "    mbert_trainer = sentiment.train(mbert_context)\n",
    "\n",
    "    #xlmr_context = context.setup_pipeline(Xlmr, require_translation = False)\n",
    "    #xlmr_trainer = sentiment.train(xlmr_context)\n",
    "    \n",
    "    #mamba_context = context.setup_pipeline(Mamba, require_translation = True)\n",
    "    #mamba_trainer = sentiment.train(mamba_context)\n",
    "else:\n",
    "    raise ValueError(\"Invalid action.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a780c5-c36a-4c4e-8611-f81bf4ec337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
