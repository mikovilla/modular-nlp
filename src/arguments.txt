Tokenizer & Vocabulary

mBERT uses a WordPiece tokenizer with a vocabulary of ~110k.

XLM-R uses a SentencePiece (BPE) tokenizer with a vocabulary of 250k.

Because of this, things like max_seq_length may behave differently depending on tokenization granularity. You don’t have to force them to be identical.

Model Size

Base mBERT ≈ 110M parameters, XLM-R base ≈ 270M, XLM-R large ≈ 550M.

Bigger models (like XLM-R large) usually require:

smaller batch size (due to memory constraints),

potentially lower learning rate or longer warmup steps.

Training Hyperparameters

Learning rate: Often similar ranges (e.g., 1e-5 – 5e-5 for fine-tuning), but XLM-R sometimes benefits from a slightly lower LR, especially for large variants.

Batch size: Should be adjusted depending on available GPU memory. Doesn’t have to match between models.

Epochs: Usually task-dependent rather than model-dependent (e.g., 3–5 epochs for classification tasks).

Warmup ratio/steps: Can remain similar, though larger models may benefit from longer warmup.

Weight decay, Adam epsilon, gradient clipping: Typically the same across Transformer models.

Task Type

If you’re fine-tuning on the same dataset and want to compare models fairly, it’s common to keep as many training arguments the same as possible (epochs, batch size, LR search space, etc.), so differences reflect the models rather than training setup.

If you’re just optimizing each model individually, you may want to tune separately.

Recommendation

For fair comparison (benchmarking): keep training arguments the same (learning rate, batch size, epochs, etc.) so results are directly comparable.

For best performance (production): tune hyperparameters separately — XLM-R and mBERT may have different optimal values because of architecture, size, and tokenization.

Task-specific tweaks

Sequence classification: the configs above are fine. Consider class-weighted loss if labels are imbalanced.

Token classification (NER/POS):

Use label_all_tokens=False (NER) and ignore subword labels via -100.

Increase num_train_epochs to 5–10 (often needs more steps).

Prefer max_seq_length=256 or 512 for long entities.

QA / long contexts:

Use max_seq_length=384 or 512 with doc_stride=128.

Consider cosine or polynomial decay schedulers; they sometimes help large models.

When to diverge between mBERT and XLM-R

Batch size: XLM-R (esp. large) typically needs smaller batches; use gradient accumulation to match effective batch if you want fairness.

Learning rate: mBERT tolerates slightly higher LR; XLM-R often prefers lower LR and longer warmup.

Epochs: set by dataset size; keep the same for fairness, but if you’re optimizing, allow XLM-R large a tad more total steps with early stopping.

Practical checklist

Set max_seq_length the same across models (128/256/512).

Use the model’s own tokenizer (AutoTokenizer.from_pretrained(model_name)).

Enable fp16=True (or bf16=True if your hardware supports it).

Use early stopping:

Log and save the effective batch size: per_device_train_batch_size * gradient_accumulation_steps * num_gpus.

Hyperparam sweep ranges worth trying:

LR: [1e-5, 2e-5, 3e-5, 5e-5] (for XLM-R large drop the top end and add 7e-6)

Warmup ratio: [0.06, 0.08, 0.1]

Epochs: [3, 4, 5]

Effective batch sizes: [16, 32, 64]

If you tell me your task (classification/NER/QA), dataset size, and GPU memory, I can tailor these to an exact, memory-safe config and propose a short sweep plan.


# Mamba

Quick rules of thumb (how Mamba differs)

Causal, no [CLS]: for classification, pool from the last non-pad token (or use a checkpoint that exposes a ForSequenceClassification head).

Tokenizer: always use the checkpoint’s own tokenizer (AutoTokenizer.from_pretrained(...)), keep padding_side="right" for causal LM fine-tuning.

Sequence length: Mamba handles longer contexts well—don’t be shy about 2k–8k if your data benefits; scale batch size accordingly.

LR:

Full fine-tune small/medium models: ~1e-5–3e-5.

LoRA/PEFT: higher works well, ~5e-5–3e-4 (most land around 1e-4–2e-4).

Warmup: a bit more than BERT is often stable (0.08–0.1).

Scheduler: linear or cosine both work; cosine w/ warmup is a nice default.

Gradient checkpointing: usually on, especially with long seqs.
LoRA tips (common defaults):

r=8–16, lora_alpha=16–32, lora_dropout=0.05–0.1

Target projection / mixer modules (whatever the HF/ssm wrapper exposes, e.g., in_proj, out_proj, x_proj, dt_proj).

Keep norms & embeddings frozen.

Data notes for causal LM:

Pack examples with efficient packing if possible (minimize padding).

Use right padding, mask loss on pads: ignore_index=-100.

For SFT/instruction data, keep EOS handling consistent with the base checkpoint.

D) Long-context settings (one of Mamba’s strengths)

Start with max_seq_length=2048. If your task clearly benefits, try 4096–8192.

Reduce batch size as seq_len grows; keep effective tokens/step roughly stable.

Always enable gradient checkpointing; consider flash-style kernels if available in your stack.

E) What to sweep (small, effective search)

LR:

Full FT: [1e-5, 2e-5, 3e-5]

LoRA: [5e-5, 1e-4, 2e-4]

Warmup ratio: [0.08, 0.1]

Epochs / steps: [3, 4, 5] (classification) or fixed total tokens (LM).

Effective batch: target 16–64 examples (classification) given your seq_len.

F) Fair comparison vs mBERT/XLM-R

If you want a clean head-to-head:

Keep epochs, effective batch size, max_seq_length, evaluation protocol, and metric the same.

Expect slight LR differences: Mamba (LoRA) usually wants higher LR than BERT-style full fine-tunes.

For classification, ensure identical label processing and use a single-vector pooling strategy for Mamba comparable to [CLS] on BERTs.